import { builtinModules, createRequire } from "node:module";
import { delimiter, dirname, extname, join } from "path";
import { FileFsRef, NodejsLambda, Span, debug, defaultCachePathGlob, download, execCommand, getEnvForPackageManager, getNodeBinPaths, getNodeVersion, getPackageJson, getScriptName, glob, isExperimentalBackendsWithoutIntrospectionEnabled, runNpmInstall, runPackageJsonScript, scanParentDirs } from "@vercel/build-utils";
import { dirname as dirname$1, extname as extname$1, isAbsolute, join as join$1, relative } from "node:path";
import { createWriteStream, existsSync, mkdtempSync, readFileSync, rmSync, unlinkSync } from "node:fs";
import { spawn } from "node:child_process";
import { tmpdir } from "node:os";
import { z } from "zod";
import { lstat, readFile, rm, writeFile } from "node:fs/promises";
import { build as build$2 } from "rolldown";
import { exports } from "resolve.exports";
import { isNativeError } from "node:util/types";
import { nodeFileTrace as nodeFileTrace$1, resolve } from "@vercel/nft";
import { transform } from "oxc-transform";
import { createRequire as createRequire$1 } from "module";
import { spawn as spawn$1 } from "child_process";
import { existsSync as existsSync$1 } from "fs";
import execa from "execa";
import { readFile as readFile$1, writeFile as writeFile$1 } from "fs/promises";

//#region src/utils.ts
async function downloadInstallAndBundle(args) {
	const { entrypoint, files, workPath, meta, config, repoRootPath } = args;
	await download(files, workPath, meta);
	const entrypointFsDirname = join(workPath, dirname(entrypoint));
	const { cliType, lockfileVersion, packageJsonPackageManager, turboSupportsCorepackHome } = await scanParentDirs(entrypointFsDirname, true, repoRootPath);
	const spawnEnv = getEnvForPackageManager({
		cliType,
		lockfileVersion,
		packageJsonPackageManager,
		env: process.env,
		turboSupportsCorepackHome,
		projectCreatedAt: config.projectSettings?.createdAt
	});
	const installCommand = config.projectSettings?.installCommand;
	if (typeof installCommand === "string") if (installCommand.trim()) {
		console.log(`Running "install" command: \`${installCommand}\`...`);
		await execCommand(installCommand, {
			env: spawnEnv,
			cwd: entrypointFsDirname
		});
	} else console.log(`Skipping "install" command...`);
	else await runNpmInstall(entrypointFsDirname, [], { env: spawnEnv }, meta, config.projectSettings?.createdAt);
	return {
		entrypointFsDirname,
		spawnEnv
	};
}
async function maybeExecBuildCommand(args, { spawnEnv, entrypointFsDirname }) {
	const projectBuildCommand = args.config.projectSettings?.buildCommand;
	if (projectBuildCommand) {
		const nodeBinPath = getNodeBinPaths({
			base: args.repoRootPath || args.workPath,
			start: args.workPath
		}).join(delimiter);
		return execCommand(projectBuildCommand, {
			env: {
				...spawnEnv,
				PATH: `${nodeBinPath}${delimiter}${spawnEnv?.PATH || process.env.PATH}`
			},
			cwd: args.workPath
		});
	}
	return runPackageJsonScript(entrypointFsDirname, ["build"], { env: spawnEnv }, args.config.projectSettings?.createdAt);
}

//#endregion
//#region src/introspection/util.ts
const BEGIN_INTROSPECTION_RESULT = "\n__VERCEL_INTROSPECTION_BEGIN__\n";
const END_INTROSPECTION_RESULT = "\n__VERCEL_INTROSPECTION_END__\n";

//#endregion
//#region src/introspection/index.ts
const require$1 = createRequire(import.meta.url);
const introspectApp = async (args) => {
	const { span } = args;
	const introspectionSpan = span.child("vc.builder.backends.introspection");
	if (isExperimentalBackendsWithoutIntrospectionEnabled()) return defaultResult(args);
	const cjsLoaderPath = require$1.resolve("@vercel/backends/introspection/loaders/cjs");
	const rolldownEsmLoaderPath = `file://${require$1.resolve("@vercel/backends/introspection/loaders/rolldown-esm")}`;
	const handlerPath = join$1(args.dir, args.handler);
	const introspectionSchema = z.object({
		frameworkSlug: z.string().optional(),
		routes: z.array(z.object({
			src: z.string(),
			dest: z.string(),
			methods: z.array(z.string())
		})),
		additionalFolders: z.array(z.string()).optional().transform((values) => {
			return values?.map((val) => {
				if (isAbsolute(val)) return relative(args.dir, val);
				return val;
			});
		}),
		additionalDeps: z.array(z.string()).optional()
	});
	let introspectionData;
	await new Promise((resolvePromise) => {
		try {
			debug("Spawning introspection process");
			const child = spawn("node", [
				"-r",
				cjsLoaderPath,
				"--import",
				rolldownEsmLoaderPath,
				handlerPath
			], {
				stdio: [
					"pipe",
					"pipe",
					"pipe"
				],
				cwd: args.dir,
				env: {
					...process.env,
					...args.env
				}
			});
			const tempDir = mkdtempSync(join$1(tmpdir(), "introspection-"));
			const tempFilePath = join$1(tempDir, "output.txt");
			const writeStream = createWriteStream(tempFilePath);
			let streamClosed = false;
			child.stdout?.pipe(writeStream);
			let stderrBuffer = "";
			child.stderr?.on("data", (data) => {
				stderrBuffer += data.toString();
			});
			writeStream.on("error", (err) => {
				debug(`Write stream error: ${err.message}`);
			});
			const timeout = setTimeout(() => {
				debug("Introspection timeout, killing process with SIGTERM");
				child.kill("SIGTERM");
			}, 8e3);
			const timeout2 = setTimeout(() => {
				debug("Introspection timeout, killing process with SIGKILL");
				child.kill("SIGKILL");
			}, 9e3);
			child.on("error", (err) => {
				clearTimeout(timeout);
				clearTimeout(timeout2);
				debug(`Loader error: ${err.message}`);
				if (!streamClosed) writeStream.end(() => {
					streamClosed = true;
					try {
						unlinkSync(tempFilePath);
					} catch (cleanupErr) {
						debug(`Error deleting temp file on error: ${cleanupErr}`);
					}
					resolvePromise(void 0);
				});
				else resolvePromise(void 0);
			});
			child.on("close", () => {
				clearTimeout(timeout);
				clearTimeout(timeout2);
				debug("Introspection process closed");
				if (!streamClosed) writeStream.end(() => {
					streamClosed = true;
					let stdoutBuffer;
					try {
						stdoutBuffer = readFileSync(tempFilePath, "utf8");
						const beginIndex = stdoutBuffer.indexOf(BEGIN_INTROSPECTION_RESULT);
						const endIndex = stdoutBuffer.indexOf(END_INTROSPECTION_RESULT);
						if (beginIndex !== -1 && endIndex !== -1) {
							const introspectionString = stdoutBuffer.substring(beginIndex + BEGIN_INTROSPECTION_RESULT.length, endIndex);
							if (introspectionString) {
								introspectionData = introspectionSchema.parse(JSON.parse(introspectionString));
								debug("Introspection data parsed successfully");
							}
						} else debug(`Introspection markers not found.\nstdout:\n${stdoutBuffer}\nstderr:\n${stderrBuffer}`);
					} catch (error) {
						debug(`Error parsing introspection data: ${error}\nstdout:\n${stdoutBuffer}\nstderr:\n${stderrBuffer}`);
					} finally {
						try {
							rmSync(tempDir, {
								recursive: true,
								force: true
							});
						} catch (err) {
							debug(`Error deleting temp directory: ${err}`);
						}
						resolvePromise(void 0);
					}
				});
				else resolvePromise(void 0);
			});
		} catch (error) {
			debug("Introspection error", error);
			resolvePromise(void 0);
		}
	});
	const framework = getFramework(args);
	if (!introspectionData) {
		introspectionSpan.setAttributes({
			"introspection.success": "false",
			"introspection.routes": "0"
		});
		return defaultResult(args);
	}
	const routes = [
		{ handle: "filesystem" },
		...introspectionData.routes,
		{
			src: "/(.*)",
			dest: "/"
		}
	];
	introspectionSpan.setAttributes({
		"introspection.success": "true",
		"introspection.routes": String(introspectionData.routes.length),
		"introspection.framework": introspectionData.frameworkSlug ?? ""
	});
	return {
		routes,
		framework,
		additionalFolders: introspectionData.additionalFolders ?? [],
		additionalDeps: introspectionData.additionalDeps ?? []
	};
};
const defaultResult = (args) => {
	return {
		routes: [{ handle: "filesystem" }, {
			src: "/(.*)",
			dest: "/"
		}],
		framework: getFramework(args)
	};
};
const getFramework = (args) => {
	try {
		let version$1;
		if (args.framework) {
			const frameworkLibPath = require$1.resolve(`${args.framework}`, { paths: [args.dir] });
			const findNearestPackageJson = (dir) => {
				const packageJsonPath = join$1(dir, "package.json");
				if (existsSync(packageJsonPath)) return packageJsonPath;
				const parentDir = dirname$1(dir);
				if (parentDir === dir) return;
				return findNearestPackageJson(parentDir);
			};
			const nearestPackageJsonPath = findNearestPackageJson(frameworkLibPath);
			if (nearestPackageJsonPath) version$1 = require$1(nearestPackageJsonPath).version;
		}
		return {
			slug: args.framework ?? "",
			version: version$1 ?? ""
		};
	} catch (error) {
		debug(`Error getting framework for ${args.framework}. Setting framework version to empty string.`, error);
		return {
			slug: args.framework ?? "",
			version: ""
		};
	}
};

//#endregion
//#region src/cervel/plugin.ts
const CJS_SHIM_PREFIX = "\0cjs-shim:";
const plugin = (args) => {
	const packageJsonCache = /* @__PURE__ */ new Map();
	const shimMeta = /* @__PURE__ */ new Map();
	const { tracedPaths } = args.context;
	const isBareImport = (id) => {
		return !id.startsWith(".") && !id.startsWith("/") && !/^[a-z][a-z0-9+.-]*:/i.test(id);
	};
	/**
	* Read and cache package.json contents
	*/
	const getPackageJson$1 = async (pkgPath) => {
		if (packageJsonCache.has(pkgPath)) return packageJsonCache.get(pkgPath);
		try {
			const contents = await readFile(pkgPath, "utf-8");
			const parsed = JSON.parse(contents);
			packageJsonCache.set(pkgPath, parsed);
			return parsed;
		} catch {
			packageJsonCache.set(pkgPath, null);
			return null;
		}
	};
	/**
	* Determine if a resolved module is CommonJS based on package.json exports
	*/
	const isCommonJS = async (bareImport, resolvedPath, resolvedInfo) => {
		const ext = extname$1(resolvedPath);
		if (ext === ".cjs") return true;
		if (ext === ".mjs") return false;
		if (ext === ".js" || ext === ".ts") {
			const pkgJsonPath = resolvedInfo.packageJsonPath;
			if (!pkgJsonPath) return true;
			const pkgJson = await getPackageJson$1(pkgJsonPath);
			if (!pkgJson) return true;
			const pkgDir = dirname$1(pkgJsonPath);
			const relativePath = resolvedPath.startsWith(pkgDir) ? resolvedPath.slice(pkgDir.length + 1).replace(/\\/g, "/") : null;
			if (!relativePath) return pkgJson.type !== "module";
			const pkgName = pkgJson.name || "";
			const subpath = bareImport.startsWith(pkgName) ? `.${bareImport.slice(pkgName.length)}` || "." : ".";
			try {
				if (exports(pkgJson, subpath, {
					require: false,
					conditions: ["node", "import"]
				})?.some((p) => p === relativePath || p === `./${relativePath}`)) return false;
				if (exports(pkgJson, subpath, {
					require: true,
					conditions: ["node", "require"]
				})?.some((p) => p === relativePath || p === `./${relativePath}`)) return true;
			} catch (err) {
				console.warn("Export resolution failed::", err);
			}
			if (pkgJson.module) return false;
			return pkgJson.type !== "module";
		}
		return true;
	};
	const isLocalImport = (id) => {
		if (id.startsWith("node:")) return false;
		if (id.includes("node_modules")) return false;
		return true;
	};
	return {
		name: "cervel",
		resolveId: {
			order: "pre",
			async handler(id, importer, rOpts) {
				if (id.startsWith(CJS_SHIM_PREFIX)) return {
					id,
					external: false
				};
				const resolved = await this.resolve(id, importer, rOpts);
				if (builtinModules.includes(id)) return {
					id: `node:${id}`,
					external: true
				};
				if (resolved?.id && isLocalImport(resolved.id)) tracedPaths.add(resolved.id);
				if (importer?.startsWith(CJS_SHIM_PREFIX) && isBareImport(id)) return {
					id,
					external: true
				};
				if (importer && isBareImport(id) && resolved?.id?.includes("node_modules")) {
					if (args.shimBareImports) {
						if (await isCommonJS(id, resolved.id, resolved)) {
							const importerPkgJsonPath = (await this.resolve(importer))?.packageJsonPath;
							if (importerPkgJsonPath) {
								const importerPkgDir = relative(args.repoRootPath, dirname$1(importerPkgJsonPath));
								const shimId$1 = `${CJS_SHIM_PREFIX}${importerPkgDir.replace(/\//g, "_")}_${id.replace(/\//g, "_")}`;
								shimMeta.set(shimId$1, {
									pkgDir: importerPkgDir,
									pkgName: id
								});
								return {
									id: shimId$1,
									external: false
								};
							}
							const shimId = `${CJS_SHIM_PREFIX}${id.replace(/\//g, "_")}`;
							shimMeta.set(shimId, {
								pkgDir: "",
								pkgName: id
							});
							return {
								id: shimId,
								external: false
							};
						}
					}
					return {
						external: true,
						id
					};
				}
				if (importer && isBareImport(id)) return resolved;
				return {
					external: true,
					...resolved,
					id: resolved?.id || id
				};
			}
		},
		load: { async handler(id) {
			if (id.startsWith(CJS_SHIM_PREFIX)) {
				const meta = shimMeta.get(id);
				if (!meta) return { code: `module.exports = require('${id.slice(10)}');` };
				const { pkgDir, pkgName } = meta;
				if (pkgDir) return { code: `
import { createRequire } from 'node:module';
import { fileURLToPath } from 'node:url';
import { dirname, join } from 'node:path';

const requireFromContext = createRequire(join(dirname(fileURLToPath(import.meta.url)), '${join$1("..", pkgDir, "package.json")}'));
module.exports = requireFromContext('${pkgName}');
`.trim() };
				return { code: `module.exports = require('${pkgName}');` };
			}
			return null;
		} }
	};
};

//#endregion
//#region src/cervel/node-file-trace.ts
const nodeFileTrace = async (args) => {
	const { span } = args;
	const files = {};
	const { tracedPaths } = args;
	const compiledSourceFiles = await glob("**/*", {
		cwd: args.outDir,
		follow: true,
		includeDirectories: true
	});
	for (const file of Object.keys(compiledSourceFiles)) files[file] = compiledSourceFiles[file];
	/**
	* While we're not using NFT to process source code, we are using it
	* to tree shake node deps, and include any fs reads for files that are
	* not part of the traced paths or compiled source files.
	* Most of this is identical to the `@vercel/node` implementation
	*/
	const runNft = () => nodeFileTrace$1(Array.from(tracedPaths), {
		base: args.repoRootPath,
		processCwd: args.workPath,
		ts: true,
		mixedModules: true,
		async resolve(id, parent, job, cjsResolve) {
			return resolve(id, parent, job, cjsResolve);
		},
		async readFile(fsPath) {
			try {
				let source = await readFile(fsPath);
				if (fsPath.endsWith(".ts") && !fsPath.endsWith(".d.ts") || fsPath.endsWith(".tsx") || fsPath.endsWith(".mts") || fsPath.endsWith(".cts")) source = (await transform(fsPath, source.toString())).code;
				return source;
			} catch (error) {
				if (isNativeError(error) && "code" in error && (error.code === "ENOENT" || error.code === "EISDIR")) return null;
				throw error;
			}
		}
	});
	const result = await span.child("vc.builder.backends.nft").trace(runNft);
	if (!args.keepTracedPaths) for (const file of tracedPaths) {
		const relativeFile = relative(args.repoRootPath, file);
		result.fileList.delete(relativeFile);
	}
	debug("NFT traced files count:", result.fileList.size);
	for (const file of result.fileList) {
		const absolutePath = join$1(args.repoRootPath, file);
		const stats = await lstat(absolutePath);
		const outputPath = file;
		if (stats.isSymbolicLink() || stats.isFile()) files[outputPath] = new FileFsRef({
			fsPath: absolutePath,
			mode: stats.mode
		});
	}
	debug("Total files in context:", Object.keys(files).length);
	return files;
};

//#endregion
//#region src/cervel/rolldown.ts
const __dirname__filenameShim = `
import { createRequire as __createRequire } from 'node:module';
import { fileURLToPath as __fileURLToPath } from 'node:url';
import { dirname as __dirname_ } from 'node:path';
var require = typeof require !== 'undefined' ? require : __createRequire(import.meta.url);
var __filename = typeof __filename !== 'undefined' ? __filename : __fileURLToPath(import.meta.url);
var __dirname = typeof __dirname !== 'undefined' ? __dirname : __dirname_(__filename);
`.trim();
const rolldown = async (args) => {
	const entrypointPath = join$1(args.workPath, args.entrypoint);
	const outputDir = join$1(args.workPath, args.out);
	const extension = extname$1(args.entrypoint);
	const extensionMap = {
		".ts": {
			format: "auto",
			extension: "js"
		},
		".mts": {
			format: "esm",
			extension: "mjs"
		},
		".cts": {
			format: "cjs",
			extension: "cjs"
		},
		".cjs": {
			format: "cjs",
			extension: "cjs"
		},
		".js": {
			format: "auto",
			extension: "js"
		},
		".mjs": {
			format: "esm",
			extension: "mjs"
		}
	};
	const extensionInfo = extensionMap[extension] || extensionMap[".js"];
	let resolvedFormat = extensionInfo.format === "auto" ? void 0 : extensionInfo.format;
	const packageJsonPath = join$1(args.workPath, "package.json");
	const external = [];
	let pkg = {};
	if (existsSync(packageJsonPath)) {
		const source = await readFile(packageJsonPath, "utf8");
		try {
			pkg = JSON.parse(source.toString());
		} catch (_e) {
			pkg = {};
		}
		if (extensionInfo.format === "auto") if (pkg.type === "module") resolvedFormat = "esm";
		else resolvedFormat = "cjs";
		for (const dependency of Object.keys(pkg.dependencies || {})) external.push(dependency);
		for (const dependency of Object.keys(pkg.devDependencies || {})) external.push(dependency);
		for (const dependency of Object.keys(pkg.peerDependencies || {})) external.push(dependency);
		for (const dependency of Object.keys(pkg.optionalDependencies || {})) external.push(dependency);
	}
	const resolvedExtension = resolvedFormat === "esm" ? "mjs" : "cjs";
	const context = { tracedPaths: /* @__PURE__ */ new Set() };
	const runRolldown = () => build$2({
		input: entrypointPath,
		cwd: args.workPath,
		platform: "node",
		tsconfig: true,
		plugins: [plugin({
			repoRootPath: args.repoRootPath,
			outDir: outputDir,
			workPath: args.workPath,
			shimBareImports: resolvedFormat === "esm",
			context
		})],
		output: {
			cleanDir: true,
			dir: outputDir,
			format: resolvedFormat,
			entryFileNames: `[name].${resolvedExtension}`,
			preserveModules: true,
			preserveModulesRoot: args.repoRootPath,
			sourcemap: false,
			banner: resolvedFormat === "esm" ? __dirname__filenameShim : void 0
		}
	});
	const out = await args.span.child("vc.builder.backends.rolldown").trace(runRolldown);
	let handler = null;
	for (const entry of out.output) if (entry.type === "chunk") {
		if (entry.isEntry) handler = entry.fileName;
	}
	if (typeof handler !== "string") throw new Error(`Unable to resolve module for ${args.entrypoint}`);
	const outputFiles = await nodeFileTrace({
		outDir: outputDir,
		tracedPaths: Array.from(context.tracedPaths),
		repoRootPath: args.repoRootPath,
		workPath: args.workPath,
		keepTracedPaths: false,
		span: args.span
	});
	const cleanup = async () => {
		await rm(outputDir, {
			recursive: true,
			force: true
		});
	};
	return {
		result: {
			handler,
			outputDir,
			outputFiles
		},
		cleanup
	};
};

//#endregion
//#region src/cervel/utils.ts
const noColor = globalThis.process?.env?.NO_COLOR === "1" || globalThis.process?.env?.TERM === "dumb";
const resets = {
	1: 22,
	31: 39,
	32: 39,
	33: 39,
	34: 39,
	35: 39,
	36: 39,
	90: 39
};
const _c = (c) => (text) => {
	if (noColor) return text;
	return `\u001B[${c}m${text}\u001B[${resets[c] ?? 0}m`;
};
const Colors = {
	bold: _c(1),
	red: _c(31),
	green: _c(32),
	yellow: _c(33),
	blue: _c(34),
	magenta: _c(35),
	cyan: _c(36),
	gray: _c(90),
	url: (title, url) => noColor ? `[${title}](${url})` : `\u001B]8;;${url}\u001B\\${title}\u001B]8;;\u001B\\`
};

//#endregion
//#region src/cervel/typescript.ts
const require_ = createRequire$1(import.meta.url);
const typescript = (args) => {
	const { span } = args;
	return span.child("vc.builder.backends.tsCompile").trace(async () => {
		const extension = extname(args.entrypoint);
		if (![
			".ts",
			".mts",
			".cts"
		].includes(extension)) return;
		const tscPath = resolveTscPath(args);
		if (!tscPath) {
			console.log(Colors.gray(`${Colors.bold(Colors.cyan("✓"))} Typecheck skipped ${Colors.gray("(TypeScript not found)")}`));
			return null;
		}
		return doTypeCheck(args, tscPath);
	});
};
async function doTypeCheck(args, tscPath) {
	let stdout = "";
	let stderr = "";
	/**
	* This might be subject to change.
	* - if no tscPath, skip typecheck
	* - if tsconfig, provide the tsconfig path
	* - else provide the entrypoint path
	*/
	const tscArgs = [
		tscPath,
		"--noEmit",
		"--pretty",
		"--allowJs",
		"--esModuleInterop",
		"--skipLibCheck"
	];
	const tsconfig = await findNearestTsconfig(args.workPath);
	if (tsconfig) tscArgs.push("--project", tsconfig);
	else tscArgs.push(args.entrypoint);
	const child = spawn$1(process.execPath, tscArgs, {
		cwd: args.workPath,
		stdio: [
			"ignore",
			"pipe",
			"pipe"
		]
	});
	child.stdout?.on("data", (data) => {
		stdout += data.toString();
	});
	child.stderr?.on("data", (data) => {
		stderr += data.toString();
	});
	await new Promise((resolve$1, reject) => {
		child.on("close", (code) => {
			if (code === 0) {
				console.log(Colors.gray(`${Colors.bold(Colors.cyan("✓"))} Typecheck complete`));
				resolve$1();
			} else {
				const output = stdout || stderr;
				if (output) {
					console.error("\nTypeScript type check failed:\n");
					console.error(output);
				}
				reject(/* @__PURE__ */ new Error("TypeScript type check failed"));
			}
		});
		child.on("error", (err) => {
			reject(err);
		});
	});
}
const resolveTscPath = (args) => {
	try {
		return require_.resolve("typescript/bin/tsc", { paths: [args.workPath] });
	} catch (e) {
		return null;
	}
};
const findNearestTsconfig = async (workPath) => {
	const tsconfigPath = join(workPath, "tsconfig.json");
	if (existsSync$1(tsconfigPath)) return tsconfigPath;
	if (workPath === "/") return;
	return findNearestTsconfig(join(workPath, ".."));
};

//#endregion
//#region src/cervel/find-entrypoint.ts
const frameworks = [
	"express",
	"hono",
	"elysia",
	"fastify",
	"@nestjs/core",
	"h3"
];
const entrypointFilenames = [
	"app",
	"index",
	"server",
	"main"
];
const entrypointExtensions = [
	"js",
	"cjs",
	"mjs",
	"ts",
	"cts",
	"mts"
];
const entrypoints = entrypointFilenames.flatMap((filename) => entrypointExtensions.map((extension) => `${filename}.${extension}`));
const createFrameworkRegex = (framework) => new RegExp(`(?:from|require|import)\\s*(?:\\(\\s*)?["']${framework}["']\\s*(?:\\))?`, "g");
const findEntrypoint = async (cwd, options) => {
	if (options?.ignoreRegex ?? false) {
		for (const entrypoint of entrypoints) if (existsSync(join$1(cwd, entrypoint))) return entrypoint;
		for (const entrypoint of entrypoints) if (existsSync(join$1(cwd, "src", entrypoint))) return join$1("src", entrypoint);
		throw new Error("No entrypoint file found");
	}
	const packageJson = await readFile(join$1(cwd, "package.json"), "utf-8");
	const packageJsonObject = JSON.parse(packageJson);
	const framework = frameworks.find((framework$1) => packageJsonObject.dependencies?.[framework$1]);
	if (!framework) {
		for (const entrypoint of entrypoints) {
			const entrypointPath = join$1(cwd, entrypoint);
			try {
				await readFile(entrypointPath, "utf-8");
				return entrypoint;
			} catch (e) {
				continue;
			}
		}
		throw new Error("No entrypoint or framework found");
	}
	const regex = createFrameworkRegex(framework);
	for (const entrypoint of entrypoints) {
		const entrypointPath = join$1(cwd, entrypoint);
		try {
			const content = await readFile(entrypointPath, "utf-8");
			if (regex.test(content)) return entrypoint;
		} catch (e) {
			continue;
		}
	}
	for (const entrypoint of entrypoints) {
		const entrypointPath = join$1(cwd, "src", entrypoint);
		try {
			const content = await readFile(entrypointPath, "utf-8");
			if (regex.test(content)) return join$1("src", entrypoint);
		} catch (e) {
			continue;
		}
	}
	throw new Error("No entrypoint found");
};

//#endregion
//#region src/cervel/index.ts
const require = createRequire(import.meta.url);
const getBuildSummary = async (outputDir) => {
	const buildSummary = await readFile$1(join$1(outputDir, ".cervel.json"), "utf-8");
	return JSON.parse(buildSummary);
};
const build$1 = async (args) => {
	const entrypoint = args.entrypoint || await findEntrypoint(args.workPath);
	const span = args.span ?? new Span({ name: "cervel-build" });
	const [, rolldownResult] = await Promise.all([typescript({
		entrypoint,
		workPath: args.workPath,
		span
	}), rolldown({
		entrypoint,
		workPath: args.workPath,
		repoRootPath: args.repoRootPath,
		out: args.out,
		span
	})]);
	await writeFile$1(join$1(args.workPath, args.out, ".cervel.json"), JSON.stringify({ handler: rolldownResult.result.handler }, null, 2));
	console.log(Colors.gray(`${Colors.bold(Colors.cyan("✓"))} Build complete`));
	return { rolldownResult: rolldownResult.result };
};
const serve = async (args) => {
	const entrypoint = await findEntrypoint(args.workPath);
	const srvxBin = join$1(require.resolve("srvx"), "..", "..", "..", "bin", "srvx.mjs");
	const tsxBin = require.resolve("tsx");
	const restArgs = Object.entries(args.rest).filter(([, value]) => value !== void 0 && value !== false).map(([key, value]) => typeof value === "boolean" ? `--${key}` : `--${key}=${value}`);
	if (!args.rest.import) restArgs.push("--import", tsxBin);
	await execa("npx", [
		srvxBin,
		...restArgs,
		entrypoint
	], {
		cwd: args.workPath,
		stdio: "inherit"
	});
};
const srvxOptions = {
	help: {
		type: "boolean",
		short: "h"
	},
	version: {
		type: "boolean",
		short: "v"
	},
	prod: { type: "boolean" },
	port: {
		type: "string",
		short: "p"
	},
	host: {
		type: "string",
		short: "H"
	},
	static: {
		type: "string",
		short: "s"
	},
	import: { type: "string" },
	tls: { type: "boolean" },
	cert: { type: "string" },
	key: { type: "string" }
};

//#endregion
//#region src/build.ts
const defaultOutputDirectory = join$1(".vercel", "node");
const doBuild = async (args, downloadResult, span) => {
	const buildCommandResult = await maybeExecBuildCommand(args, downloadResult);
	const outputSetting = args.config.outputDirectory;
	const buildCommand = args.config.projectSettings?.buildCommand;
	const isCervelCommand = buildCommand?.trim().startsWith("cervel");
	if (!outputSetting) {
		debug("No output directory configured, using default output directory");
		if (isCervelCommand) {
			debug("Cervel command ran, using its default output location");
			const cervelOutputDir = join$1(args.workPath, "dist");
			if (existsSync(join$1(cervelOutputDir, ".cervel.json"))) {
				debug("Cervel JSON file found, using its handler");
				const { handler: handler$2 } = await getBuildSummary(cervelOutputDir);
				return {
					dir: cervelOutputDir,
					handler: handler$2
				};
			}
			throw new Error(`Build command "${buildCommand}" completed, but no output was found at ${cervelOutputDir}. Make sure your cervel command is configured correctly.`);
		}
		const distDir = join$1(args.workPath, "dist");
		if (existsSync(distDir)) {
			debug("Dist directory found, checking for .cervel.json");
			const cervelJsonPath$1 = join$1(distDir, ".cervel.json");
			if (existsSync(cervelJsonPath$1)) {
				const { handler: handler$3 } = await getBuildSummary(distDir);
				return {
					dir: distDir,
					handler: handler$3
				};
			}
			let handler$2;
			try {
				debug("Finding entrypoint in dist directory");
				handler$2 = await findEntrypoint(distDir);
			} catch (error) {
				try {
					debug("Finding entrypoint in dist directory with ignoreRegex");
					handler$2 = await findEntrypoint(distDir, { ignoreRegex: true });
					debug("Found entrypoint in dist directory with ignoreRegex", handler$2);
				} catch (error$1) {
					debug("Unable to detect entrypoint, building ourselves");
					const buildResult$1 = await build$1({
						workPath: args.workPath,
						repoRootPath: args.repoRootPath,
						out: defaultOutputDirectory,
						span
					});
					const { handler: handler$3 } = await getBuildSummary(buildResult$1.rolldownResult.outputDir);
					return {
						dir: buildResult$1.rolldownResult.outputDir,
						handler: handler$3,
						files: buildResult$1.rolldownResult.outputFiles
					};
				}
			}
			await writeFile(cervelJsonPath$1, JSON.stringify({ handler: handler$2 }, null, 2));
			const files = await nodeFileTrace({
				keepTracedPaths: true,
				tracedPaths: [join$1(distDir, handler$2)],
				repoRootPath: args.repoRootPath,
				workPath: args.workPath,
				outDir: distDir,
				span
			});
			return {
				dir: distDir,
				handler: handler$2,
				files
			};
		}
		debug("No dist directory found, or unable to detect entrypoint, building ourselves");
		const buildResult = await build$1({
			workPath: args.workPath,
			repoRootPath: args.repoRootPath,
			out: defaultOutputDirectory,
			span
		});
		const { handler: handler$1 } = await getBuildSummary(buildResult.rolldownResult.outputDir);
		return {
			dir: buildResult.rolldownResult.outputDir,
			handler: handler$1,
			files: buildResult.rolldownResult.outputFiles
		};
	}
	const outputDir = join$1(args.workPath, outputSetting);
	const packageJson = await getPackageJson(args.workPath);
	const monorepoWithoutBuildScript = args.config.projectSettings?.monorepoManager && !getScriptName(packageJson, ["build"]);
	if (!buildCommandResult || monorepoWithoutBuildScript) {
		const buildResult = await build$1({
			workPath: args.workPath,
			repoRootPath: args.repoRootPath,
			out: outputDir,
			span
		});
		const { handler: handler$1 } = await getBuildSummary(buildResult.rolldownResult.outputDir);
		return {
			dir: buildResult.rolldownResult.outputDir,
			handler: handler$1,
			files: buildResult.rolldownResult.outputFiles
		};
	}
	const cervelJsonPath = join$1(outputDir, ".cervel.json");
	if (existsSync(cervelJsonPath)) {
		const { handler: handler$1 } = await getBuildSummary(outputDir);
		return {
			dir: outputDir,
			handler: handler$1
		};
	}
	let handler;
	try {
		handler = await findEntrypoint(outputDir);
	} catch (error) {
		handler = await findEntrypoint(outputDir, { ignoreRegex: true });
	}
	await writeFile(cervelJsonPath, JSON.stringify({ handler }, null, 2));
	return {
		dir: outputDir,
		handler
	};
};

//#endregion
//#region src/index.ts
const version = 2;
const build = async (args) => {
	const downloadResult = await downloadInstallAndBundle(args);
	const nodeVersion = await getNodeVersion(args.workPath);
	const builderName = "@vercel/backends";
	const span = args.span ?? new Span({ name: builderName });
	span.setAttributes({ "builder.name": builderName });
	const entrypoint = await findEntrypoint(args.workPath);
	debug("Entrypoint", entrypoint);
	const buildSpan = span.child("vc.builder.backends.build");
	const introspectionSpan = span.child("vc.builder.backends.introspectApp");
	const [buildResult, introspectionResult] = await Promise.all([buildSpan.trace(() => doBuild(args, downloadResult, buildSpan)), introspectionSpan.trace(() => introspectApp({
		handler: entrypoint,
		dir: args.workPath,
		framework: args.config.framework,
		env: {
			...args.meta?.env ?? {},
			...args.meta?.buildEnv ?? {}
		},
		span: introspectionSpan
	}))]);
	const files = buildResult.files;
	const { routes, framework } = introspectionResult;
	if (routes.length > 2) debug(`Introspection completed successfully with ${routes.length} routes`);
	else debug(`Introspection failed to detect routes`);
	const handler = buildResult.handler;
	if (!files) throw new Error("Unable to trace files for build");
	const lambda = new NodejsLambda({
		runtime: nodeVersion.runtime,
		handler,
		files,
		shouldAddHelpers: false,
		shouldAddSourcemapSupport: true,
		framework: {
			slug: framework?.slug ?? "",
			version: framework?.version ?? ""
		},
		awsLambdaHandler: "",
		shouldDisableAutomaticFetchInstrumentation: process.env.VERCEL_TRACING_DISABLE_AUTOMATIC_FETCH_INSTRUMENTATION === "1"
	});
	const output = { index: lambda };
	for (const route of routes) if (route.dest) {
		if (route.dest === "/") continue;
		output[route.dest] = lambda;
	}
	return {
		routes,
		output
	};
};
const prepareCache = ({ repoRootPath, workPath }) => {
	return glob(defaultCachePathGlob, repoRootPath || workPath);
};

//#endregion
export { build, build$1 as cervelBuild, serve as cervelServe, findEntrypoint, getBuildSummary, introspectApp, nodeFileTrace, prepareCache, srvxOptions, version };